# Concurrent TCP socket server

## Описание

Проектът реализира многонишков сървър, предоставящ възможност за сортиране на масив от цели числа. Текущата имплементация е на C++, работи с тип int, като са имплементирани и помощни функции за сериализация на 16, 32 и 64-битови цели числа със или без знак. Алгоритъмът за сортиране е паралелен quicksort, имплементиран чрез OpenMP. Предоставени са и имплементации на стандартен еднонишков (последователен) quicksort, а също и на паралелен такъв, но посредством предварително зададен "басейн" от нишки (thread pool), като последния има явни проблеми при сортиране на повече от 20 хиляди елемента (за пръв път използвах std::future и останалия подобен набор от функции в стандартната библиотека, най-вероятно нещо не съм направил както трябва).

## Процес на разработка

Процесът на разработка може да се раздели на следните няколко стъпки:

### Архитектура

Първата работа по проекта беше избора архитектура на сървъра. Изборът се свеждаше до няколко опции:

* Еднонишков сървър, който да обработва максимум един клиент по едно и също време - това значително ограничава клиента, тъй като той ще трябва да чака своя ред, за да използва изчислителната способност на сървъра

* Еднонишков сървър с мултиплексиран неблокиращ (non-blocking) вход/изход - използват се инструменти като select, poll, epoll за unix и т.н. Всеки от тях има своите предимства и недостатъци: select например позволява да следим множество на брой файлови дескриптори, като ни информира колко от тях за готови за четене, но не и кои всъщност са готови, т.е. ние сме длъжни да минем през всички и да намерим тези, които са готови.

* Многонишков сървър, който обособява отделна нишка за всеки клиент - чрез този метод лесно ще можем успоредно да обслужваме голям брой клиенти, но създаването на нишка, а след приключването на работата й и нейното унищожаване, са скъпи операции, които е по-добре да се избягват. Разбира се и множеството нишки бързо ще изразходят хардуерните ресурси на системата.

* Многонишков сървър с обособен краен брой нишки (thread pool) - използвайки този вариант ограничаваме броя нишки до оптимално откъм производителност ниво, а също така и премахваме проблема със скъпото създаване и унищожаване на нишки, тъй като крайния набор от нишки се инициализира предварително и след това само се възръчват задачи на отделните нишки.

Разбира се, най-вероятно има и други парадигми, които може да се използват, които може би са по-сложни и най-вероятно не съм и срещал (вероятно нещо свързано с асинхронен вход/изход е добро предположение), но аз се ограничих до избор между горните четири. В действителност планирах да съчетая втория и последния вариант, като по този начин да разполагам с краен набор от нишки за обслужване на клиенти, като всяка нишка да използва select/poll/epoll за да обработва заявки от няколко клиента наведнъж. Това е най-оптималният вариант, за който можах да се сетя - след малко ровене открих и че популярни технологии като nginx работят на подобен принцип. Въпреки че е най-доброто решение, което открих, също така щеше да бъде и най-сложно за имплементация, затова реших за момента да имплементирам единствено многонишковия модел с thread pool, а в бъдеще евентуално да си поиграя с неблокиращ мултиплексиран вход и изход.

### Базова имплементация на сървър и клиент

#### Thread pool

Тъй като избрах последния от описаните по-горе варианти за архитектура на сървъра, започнах с имплементация на сравнително прост thread pool. За целта имплементирах thread-safe опашка като структура от данни, която да съхранява задачите за нишките-работници. Тя използва std::queue за контейнер и std::mutex за контрол и синхронизация на достъпа от множеството нишки. Класът thread_pool инициализира вектор от нишки при създаването си и задава всяка нишка да следи опашката за нови задачи. Предоставя два основни метода за добавяне на задачи за изпълняване от нишките - enqueue_work и enqueue_task. Първият от тях добавя съответната функция в опашката и не връща нищо, тоест няма възможност за следене на резултата от изпълнението на задачата. Вторият, enqueue_task, е опит за употреба на std::future и std::packaged_task, който да предоставя достъп до резултата от изпълнението на функцията или дали тя е завършила. При употребата на този метод в една от имплементациите на quicksort, които направих, стигнах до извода, че може би методът enqueue_task не е особено надежден в момента и често стига до segmentation fault.

#### Сървър и клиент

След като вече имах достатъчно добре работещ thread pool, преминах към имплементация на базов Hello World сървър, към който клиент да може да се свърже и да получи от сървъра поздрав от сорта на "Hello, client". За целта използвах системните извиквания за работа със сокети в unix, предоставени в <sys/socket.h>. Сървърът инициализира socket за приемане на връзки от нови клиенти, както и thread pool, в който създава задача за всеки клиент, която да се грижи за обработката на заявката на клиента - това първоначално се изразяваше в изпращането на някакво съобщение. Успоредно с това имплементирах и простичък клиент, който също изпозлва thread pool за тестване на множество на брой паралелни връзки към сървъра.

#### Алгоритъм за сортиране

След като имах работещи сървър и клиент, следваше същинската работа върху алгоритъмът, който сървърът да изпълнява върху данните, пристигащи от клиента. Избрах да имплементирам quicksort. За целта направих имплементация на последователен quicksort върху масив от int стойности. По-късно добавих и същата имплементация, работеща с std::vector и итератори. След това преминах към паралелен quicksort. Реших да пробвам две версии, една от които да използва моят thread pool, а другата - OpenMP директиви. Започнах с OpenMP, тъй като ми се стори сравнително лесно и бързо за имплементиране, тръгвайки от последователната версия като отправна точка. След това създадох и версията с thread pool. Създадох прост bash скрипт за генериране на файл с произволни числа, като техния брой се подава като аргумент на скрипта. Заедно с това създадох и тестови програми за тестване на всяка от версиите на алгоритъма, които да показват времето за изпълнението на алгоритъма. Програмите четат набор от числа от файл, генериран чрез bash скрипта, и изпълняват алгоритъма, като за тестване пренасочвах изхода на всяка от тях в изходен файл - всяка от програмите извежда както сортирания набор от числа, така и времето за изпълнение. При използването на тестовите програми установих и проблемът с thread pool имплементацията си, тъй като в тази версия на quicksort използвам преимуществата на std::future, за да следя кои от задачите са готови, за да може чакащите нишки да не седят без работа, а да поемат от нераздадените задачи. Тъй като thread pool версията на quicksort използва enqueue_task метода, който е проблемен към момента, тя става неработоспособна при входове от порядъка на 30 хиляди числа. OpenMP версията работи коректно - тя и последователната версия на алгоритъма са тествани с 10 милиона записа, като OpenMP версията е поне 2 пъти по-бърза. Крайната имплементация на сървъра използва именно тази версия.

#### Протокол

Следващата стъпка е избора на метод за сериализиране на данните, които се обменят между сървъра и клиента. За целта, след кратоко ровене и проучване в интернет, създадох следния опростен модел на пакет:

#-------------------------------------------------------#  
| PACKET LENGTH | ARRAY SIZE | ARRAY OF INTEGERS (DATA) |  
#-------------------------------------------------------#  

* PACKET LENGTH - 4 bytes
* ARRAY SIZE - 4 bytes
* ARRAY OF INTEGERS (DATA) - 1492 bytes
* MAX PACKET SIZE - 1500 bytes

Големината на данните в пакета се съдържа в PACKET LENGTH, за което са отредени 4 байта. Броя на числата в масива се съдържа в следващите четири байта (ARRAY SIZE). След това съм избрал да отделя максимум 1492 байта за самия масив от числа, като това прави общо 1500 байта за целия пакет, което е MTU на интерфейсите на машината ми по подразбиране (с изключение на loopback).  

Тъй като целият пакет се състои единствено от цели числа за данни, то основно ми трябваха помощни функции за сериализация на такива. Във файла pack.h са налични имплементации на функции за сериализация на 16, 32 и 64-битови цели числа със и без знак в масив от символи - за целта всеки байт от числото се пази в един char - тоест 16-битовото число се сериализира в масив от два символа (считам, че всеки символ е 1 байт, надявам се това да е стандартно на повечето платформи).

#### Финална имплементация на сървъра и клиента

След като бях приготвил всичко необходимо откъм сериализация на данни, създаване на така дефинираните в моя протокол пакети и имах нужната имплементация на алгоритъма, добавих логиката за изпращане на std::vector от цели числа от клиента, като предварително го сериализирам и пакетирам, сървърът приема буфер от данни, отваря пакета, десериализира го до std::vector от цели числа, сортира го посредством паралелния quicksort алгоритъм и връща резултата на клиента по същиата процедура. В имплементацията съм приел да използвам 16 битови числа, затова всеки вход и изход се сериализира и десериализира до такъв. Не успях да измисля особено гъвкав начин да се поддържат всички видове числа едновременно.

### Употреба

#### Изисквания

Проектът е реализиран на C++ и се изгражда чрез CMake. Разработен е на Fedora 37 с компилатор gcc, тестван е само на Fedora, като до някакъв момент и на Ubuntu 20.04, но беше в първоначалните етапи.

Версии на инструментите:

* gcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-4)
* cmake version 3.25.1

Проектът ипозлва единствено fmt като библиотека за форматиране на изхода. Не би трябвало да има нужда от някакви допълнителни стъпки за инсталацията й.

#### Изграждане на проекта

За изграждане на проекта изпълнете следните команди в директорията на проекта посредством терминал:

```
cmake -S . -B build
cmake --build build

./build/server <portnum>
./build/client <hostname> <portnum> <instances>
```

Пример:

```
./build/server 5555
./build/client localhost 5555 3
```

За преглед на резултатите от тестовете на различните версии на quicksort алгоритъма, може да използвате предоставените bash скриптове. Трябва първо да използвате generate_random.sh за генериране на входящ файл с числа за сортиране, а после да изпълните test.sh, което ще изгради проекта и ще изпълни два от изходните изпълними файлове - OpenMP версията и Sequential версията. Изходът ще покаже времето за изпълнение на двата алгоритъма.

Забележка: Тестовите програми очакват входящия файл с числа да се казва "input". За жалост това е hard-coded и не ми остана време да се позанимавам да го направя по-адекватно. Също така броят числа, които очакват файлът да съдържа, са директно зададени в кода, тоест ако искате да използвате конкретна бройка числа, различна от примера по-долу, ще се наложи да направите промени във cpp файла.

Забележка: Може да се наложи двата bash скрипта да трябва да се настроят с права за изпълнение, за да ги използвате.

Пример:

```
./generate_random.sh 1000000 > input
./test.sh
```

